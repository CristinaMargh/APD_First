# APD_First
### Map-Reduce Operations

Regarding the general structuring of the code, the program begins by
defining several structures, of which I mention: the first one necessary
to store each word and its associated data, such as: the word
itself, a vector that will store the IDs of the files in which it appears,
the number of files in which it is present or the total number of occurrences in
all files.
For more efficient and faster management of the implementation, we used a
HashMap structure that contains a mutex for each bucket in the hashmap and the
table.
The processing of the input file list was done using the
FileList structure. This also contains a mutex to protect access.
The management of the results generated by the reduce and map operations was
done using the MapperArgs and ReducerArgs structures. For synchronization between
them, we used a barrier. Also, for the reducer threads I also use a mutex vector for letters.

For parsing and working with files I used auxiliary functions for
initialization and for obtaining the next file that needs to be
processed by the thread. Access was controlled by using a mutex.
Regarding the hash function, it uses a well-known algorithm.
The implementation is efficient because it uses both addition and shifting.
At initialization I allocated memory for the HashMap structure and for the hash
table which is actually a vector of Node pointers. Finally I gave
each bucket in the hash table the value NULL and initialized the mutexes.
Another important part is adding a word to the hash map along with the
ID of the file where it was found. If we already had the word we need to make the
necessary updates. Otherwise, we have a new word so we allocate memory for it and initialize it.
For the mapper function, we process the files and store the words. When
we read words from the file we need to remove non-alphabetic characters
and convert them all to lowercase according to the instructions in the statement.
For this we created 2 helper functions. Finally we wait at the barrier
for the mappers to finish.
For the reducer we need to determine the letters to be processed.
We find the total number of nodes that start with the letter assigned to the reducer.
Then we sort the Node structures that we found out that start with
the letters of our reducer. We used a sorting function in qsort so that
all unique words are sorted in descending order by the number of the input file
in which they appear and in case of equality alphabetically. Then we write them to the output file.

In order not to have problems with memory management, we built auxiliary functions
for freeing it in hash map (freeing on the table, lock vectors
and structure) and file list. Finally, we destroy the mutex.

In the main function, we parsed the arguments, initialized the structures,
and then for the parallelization part, we created the threads for
the map and reduce operations.
